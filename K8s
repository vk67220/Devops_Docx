Kubernetes:-  Orchestration platform

K8S is an orchestration platform means management.by using K8S we can manage
     - Docker containers
To automate the management of containers have to use Orchestration engines.
     -Docker swarm (outdated)
     -k8s(advanced version)
k8s is used to manage the containers of our app.
k8s will take care of containers deployment,scaling,de-scaling and containers load balancers

scaling the containers in docker swarm is manual process 

production related deployments we will use k8s and it is highly recommended


Clusters

k8s will run based on cluster
cluster is nothing but group of servers
one server is called as master node or control plane
remaining servers are called as worker nodes

Master node acts as a manager, worker nodes acts as team members







K8s architecture:-

in k8s most important component is master node/control plane

using 'kubectl' we can communicate with the Control Plane

in control plane we will have

API server 
scheduler
controller manager
etcd

worker nodes

kubelet --worker node agent
kube-proxy


A DevOps engineer submits a deployment request to the API server, which stores the request in etcd, the internal database of the Kubernetes cluster.

The API server is responsible for handling incoming requests from users and storing them in etcd.

The scheduler identifies pending requests in etcd and determines which worker node should execute the task. It makes this decision by communicating with kubelet.

The controller manager receives information from the scheduler via kubelet and instructs the worker node to create the container.

The controller manager is responsible for managing task execution on the worker node.

Within the pod, the container is created and runs the application.

The kube-proxy is a networking component in Kubernetes that maintains network rules on each node, enabling communication between services and pods within the cluster.



Kubernetes components:-

Pods:-

In K8S every container will be created inside the pod
Pod always runs inside the worker nodes
We can create multiple pods in a single node
Every pod will have unique IP address
whenever a pod dies, a new IP address will be created


How pods will communicate?

Each pod gets its own IP address. So each pod can communicate with each other using their internal IP address

K8S Manifest File
In Manifest yml file, there are 4 sections available;
	1. apiVersion
	2. kind
	3. metadata
	4. spec


Can we access the app running inside a pod?
NO. We cannot access app outside the pod. Because pods cannot be accessed outside of the cluster. By default, in K8S pods are specific to cluster.

To access the pods outside of the cluster, we need to expose the pods for outside access.
We will use K8S "Service" concept to expose the pods outside of the cluster

K8S Service concept is used to expose the pods outside of the cluster.
We have 3 types of K8S Services to expose the pods outside of cluster;
	1. Cluster IP
	2. Node Port
	3. Load Balancer

ClusterIP:

To access the pods within the cluster, we will use Cluster IP service
in Cluster IP service all the pods are mapped to single IP address, which is called as Static IP
expose the pods within the cluster, we can use Cluster IP service
This will generate Static IP to access the pods within the cluster
Cluster IP will not change when the pods are recreated

2. NodePort
Node Port is a service which is used to expose the pod outside of the cluster
When we are using Node Port service, we will specify the type as "NodePort" in service manifest file
if we dont specify the NodePort number, K8S will provide a random ip using which we have to access the app.
If we want to run our app on our own port number then specify the port number under nodePort in the service manifest file

any number can be given but it has to be b/w 30000 and 32767

3. Load Balancer

Load Balancer will distribute the traffic equally to all the Worker Nodes available in the cluster

Namespace:

Namespace is used to group the K8S components logically
We can create multiple namespaces in K8S cluster
Namespaces are isolated
Whenever we create the pods under a namespace, when we delete that namespace, all the pods inside that namespace will also gets DELETED


Always we need to create the pods using below K8S components;
1. Replication Controller
2. Replica Set
3. Deployment
4. Stateful Set
5. Daemon Set

If you create the pods using above K8S components, K8S will provide self healing capability. If you pod is crashed, K8S will recreate it.


1. Replication Controller (RC)
It is a K8S component to create pods
if i ask to create 5 pods for my app, if any one pod gets crashed, replication controller will create a new pod to match the given number of pods i.e 5
RC will take care of pod lifecycle
using RC, we can scale up and scale down the pods.

Note: if you delete the Replication Controller, the replicas (pods) will also get deleted.

2. Replica Set (RS)

The only difference b/w RC and RS is in 'selector'
Selector is used to identify the pod using 'pod label'
RC supports equality based selector
RS supports set based selector

RC:
selector:
	app: javawebapp

RS:
selector:
	matchLabels:
		app: javawebapp
		version: v1
		type: backend

Equality based selector means if pod label matches with 'selector' then only the pod will be identified
In RC, we can configure only one pod label. But in RS, our pods can be made identified by using multiple pod labels. This is called as Set based selector.

3. Deployment
It is also one of the K8S component
It is the most recommended approach to deploy the app in the K8S cluster
In RS & RC scale up and scale down is a manual process
Using 'deployment', scale up and scale down is possible in an automated way i.e Auto Scaling
Deployment also supports Roll Out and Roll Back
	

In Replication controller to deploy the latest code we have to delete RC & RS, then pods will also be deleted and app will go down.
In deployment, we can deploy the latest code with zero downtime for the application i.e HA

In Realtime we prefer to work with 'deployment' concept;
	1. app will have zero downtime
	2. we can easily roll out and roll back
	3. autoscaling can be achieved based on traffic to the app

Deployment strategies;

In K8S to deploy our app in K8S cluster we have 2 strategies
	1. ReCreate - it will delete all the existing pods at a time and will create new pods. Here downtime for the app is possible
	2. Rolling Update - it will delete the pods one by one and will create new pods simultaneously - mostly preferred in the realtime

In deployment manifest file, if we dont specify deployment strategy, by default K8S will consider as 'Rolling Update'


Config Map & Secrets
=====================
Config Map & Secret concepts are used to avoid the hard coded properties in the application.

Config Map is used to store the data in the key-value format (non-confidential data).
Ex:-Environment variables, Config files	

For non-confidential data (usernames) we will use ConfigMap.
For confidential data (passwords) we will use Secret.

Secret is used to store confidential data in key-value format. Here we will store the data in encrypted format
Kubernetes stores secrets Base64-encoded.
Ex:-Database credentials, API keys

ConfigMap and Secret will make docker images as portable

How to read the data from ConfigMap?
We have to write a yml to read the data from configmap yml
	- to create the MySQL container i need the data from config map and secret.

K8S Volumes

in docker, we have used docker volumes concept. The purpose of volumes is to store the data. To make our containers stateful we will use storage. 
Similarly in K8S our pods are stateless. whatever the data that is generated in the app, i want to store the data permanently. For that in K8S we have a concept called PV - Persistent Volumes


persistentVolumeReclaimPolicy 3 types

Retain
Delete
Recycle 


--------------------------------------------------------
Stateful Containers & Stateless Containers
--------------------------------------------------------
stateful container maintains the data permanently
stateless container wont maintains the data permanently

In stateless containers based apps for every request they will get new data and it will process that data
	Ex: nodeJS, Nignx, SpringBoot...
	Every request that is coming to the app will be treated as a new request
In stateful containers based apps they will keep track of data
	Ex:MySQL, Oracle, Postgress

In K8S we have 2 important concepts;
	1. deployment - used for stateless apps
	2. statefulset - used for stateful apps
If you want to deploy the DB in the EKS cluster, 'statefulset' is recommended
If you are deploying the frontend of app, we will use stateless app i.e 'deployment'

Deployment Vs StatefulSet
--------------------------------------
Deployment will create the pods with Random IDs/Names
Deployment will scale down the pods in random order
Deployment pods are stateless pods

StatefulSet will create the pods with sticky identity
StatefulSet will scale down the pods in reverse order
StatefulSet pods are stateful pods

2 Pods DB - Total 5 pods for DB --- replica = 7

A statefulset is the K8S controller that is used to run the stateful app as containers (pods) in the cluster

Daemon Set is a K8S resource to create pods
Daemon Set will create one pod in each worker node based on replica count
In real-time Daemon Set is used for logs monitoring
Daemon Set is used to setup ELK (E - Elastic Search, L - Logstash, K - Kibana)

when to use daemon Set?
	1. Logs collector
	2. Node monitoring


Helm charts:-

chart.yml -->contains info about the chart,dependencies and version
license---> contains license info
chart-->detailed info about chart
values.yml --> values which need to be passed to manifest files
charts -->dependencies
templates --> our manifest file which were templated


namenode :-nodeName is used to force a Pod to run on a specific node.
nodeselector:-nodeSelector is used to tell a Pod to run on a node that has specific labels.
         ----If no node has the label, the Pod stays in Pending state.
Taints:-taints are used to prevent Pods from running on specific nodes
     types----noschedule,prefernoschedule,noexecute
Tolerations:-tolerations allow Pods to run on nodes that have taints.


PodAffinity:-
Pod Affinity means keeping certain Pods together on the same node or close to each others 

Types of Pod Affinity
1️⃣ requiredDuringSchedulingIgnoredDuringExecution
2️⃣ preferredDuringSchedulingIgnoredDuringExecution

Advantages
Better performance by keeping related Pods close.
Faster communication between services (low latency).
e.g., frontend & backend on the same node.

Pod Anti-Affinity:-
Pod Anti-Affinity ensures that certain Pods do not run together on the same node.
Advantages
Distributes Pods across multiple nodes.

NodeAffinity:-It helps in placing Pods on specific nodes

First, label the nodes
Now, apply Node Affinity in the Pod specification

pod restart policies:-

restartPolicy: always (default)
restartPolicy: OnFailure
restartPolicy: Never


---------------------
1. Autoscaling
---------------------
It is the process of increasing or decreasing the infrastructure or resources based on demand
Increasing or decreasing the pods manually is a complex process - Autoscaling
Types of autoscaling;
	1. Horizontal Autoscaling - increasing the no of resources - preferred
	2. Vertical Autoscaling - increasing the capacity of single system

	1. HPA - Horizontal Pod Autoscaling - preferred
	2. VPA - Vertical Pod Autoscaling

RS & RC depends on CPU/Memory utilization
HPA will interact with 'metric server' to identify CPU/Memory utilization of POD
Metric Server is an app that collects metrics from objects such as PODS, Nodes a/c to the state of CPU/Memory
Metric Server can be installed in the system as an add-on

Liveness Probe:

Checks if the pod is still alive.
If it fails, Kubernetes kills and restarts the pod.

This probe checks the /healthz

Readiness Probe:

Checks if the pod is ready to handle traffic.
If it fails, Kubernetes removes the pod from the load balancer (but doesn’t restart it).

This probe checks the /ready


Blue envi   - older version of code
Green envi  - newer version of code
