---
apiVersion: v1
kind: Pod #Kind is written as Pod, because we are creating a POD here
metadata:
  name: javawebapppod #its a pod name
  labels: #to identify pods in the cluster, we will give labels. Label is an ID for the Pod
    app: javawebapp #pod label
spec: #here we will specify what we want to do with pod
  containers: #here we are creating a container
    - name: javawebappcontainer #container name
      image: kastrov/javaappkastro
      ports: #port mapping
        - containerPort: 8080 #any number can be given
---

---
apiVersion: v1
kind: Service
metadata: 
  name: javawebappsvc #Service name. Any name can be given
spec: # used to identify the lable of a pod
  type: NodePort
  selector: # it is used to identify the pod by using pod label
    app: javawebapp
  ports: 
  - port: 80 #serivce cluster host port
    targetPort: 8080 #container port
---

Manifest file (2 in 1): Pod and service manifests are in the same file
---
apiVersion: v1
kind: Pod
metadata: 
  name: javawebapppod
  labels:
    app: javawebapp
spec:
  containers:
    - name: javawebappcontainer
      image: kastrov/zomato
      ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata: 
  name: javawebappsvc
spec:
  type: NodePort
  selector: 
    app: javawebapp
  ports: 
    - port: 80
      targetPort: 3000
      nodePort: 31877 #any number can be given but it has to be b/w 30000 and 32767
---

Create the pod
Check in which worker node the pod is created
Open the port given in above service manifest file of worker node security group
access the app: http://13.127.196.123:31877/


Namespace with service loadbalancer
---
apiVersion: v1
kind: Namespace
metadata:
   name: kastrons
---
apiVersion: v1
kind: Pod
metadata:
  name: javawebapppod
  labels:
    name: javawebapp
  namespace: kastrons 
spec:
  containers:
  - name: javawebappcontainer
    image: kastrov/swiggy
    ports:
     - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
  namespace: kastrons
spec:
  type: LoadBalancer
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31700
 ---

Replication Controller

---
apiVersion: v1
kind: ReplicationController
metadata:
  name: javawebapprc
spec:
  replicas: 3
  selector:
    app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
        - name: javawebappcontainers
          image: kastrov/swiggy
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 30999
 ---

ReplicaSet

---
apiVersion: v1
kind: RelicaSet 
metadata:
  name: javawebapprs
spec:
   replicas: 3
   selector:
     matchLabels:
       app: javawebapp
     template:
       metadata:
         name: javawebapppod
         labels:
           app: javawebapp
       spec:
         containers:
           name: javawebappcontainer
           image: kastrov/bms
           ports:
             containerport: 3000
---
apiVersion: v1
kind: apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector: 
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31000
---


Deployment

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  strategy:
    type: ReCreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      labels:
        app: javawebapp
    spec:  
      containers:
      - name: javawebappcontainer
        image: kastrov/zomato
        ports:
        - containerPort: 3000

---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31000
---

above script do statefulset 


Helm charts:-

create a testdemo folder insdie create a folder called template
testdemo
   |------chart.yml
   |------values.yml 
   |------template
            |--------- nginxpod.yml ----manifest file
nginxpod.yml create in template folder


apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       image: nginx:latest
     - conatinerPort: 80


apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       image: {{ .Values.nginx.tag }}
     - conatinerPort: {{ .Values.nginx.nginxport }}

apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       {{- with.Values.nginx }}
       image: nginx: {{ .tag }}
     - conatinerPort: {{ .nginxport }}
     {{- end}}


if we comment tag in values-prod.yml file

apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       {{- with.Values.nginx }}
       image: nginx: {{ .tag | default "test" }}  if i comment tag details on values-prod.yml it will take test as tag name
     - conatinerPort: {{ .nginxport }}
     {{- end}}

values.yml

nginx:
 tag: latest
 nginxport: 80

values-prod.yml

nginx:
 tag: v1
 nginxport: 8080

chart.yml

apiVersion: v2
name: nginx-deploy
description: "Deploying nginx chart to K8S cluster"
version: 0.0.1
appVersion: latest


How to setup HPA in K8S Cluster?
1. Install metric server/API
2. Deploy a sample app
3. Create Service
4. Deploy HPA
5. Increase load - load generator
6. Monitor HPA events

To increase the load on server manually
kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://hpa-demo-deployment; done"



------------------------------------
K8S Web Dashboard
------------------------------------
1. Kubectl (CLI)
2. Web Dashboard (UI based)

======================
K8S Web Dashboard Setup
======================
Connect to Master Node
kubectl delete all --all

#Lets download and apply the yml file to set web dashboard
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml

# The above yml file will create a namespace called "kubernetes-dashboard," where the pods get created.
# Lets see the pods that are running in the namespace
$ kubectl -n kubernetes-dashboard get pods -o wide
#You can see 2 pods got created.

# Lets see the service that is created in the namespace
$ kubectl -n kubernetes-dashboard get svc

# Edit k8s dashboard service and change it to NodePort from Cluster IP
$ kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard

Note: Check kubernetes-dashboard node port and enable that Node PORT in security group of worker node
#To see the details of port no.
$ kubectl -n kubernetes-dashboard get svc
#You will see port no. as 443:30077/TCP
Goto worker node VM and edit the inbound SG rules and add the port 30077 with custom TCP and anywhere IPv4
Goto worker node VM and edit the inbound SG rules and add the port 443 with HTTPS and anywhere IPv4

# Check in which node kubernete-dashboard POD is running
$ kubectl get pods -o wide -n kubernetes-dashboard


# Access k8s web ui dashboard using below URL

URL : https://node-public-ip:30077/
#You will see the web dashboard

#'Check' Token. In order to get the token we need to work with RBAC (Roll Based Access). So we need to create RBAC i.e we need to create an user and for that user we need to give access for web dashboard.

#Lets create admin user with below yml

$ vi create-user.yml

#Copy and paste the below yml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard  #We are creating the 'admin-user' under the "kubernetes-dashboard" namespace.
...

$ kubectl apply -f create-user.yml
#You can see admin-user created.

Now we need to create the role and bind that role to the admin-user

# create cluster role binding
$ vi cluster-role-binding.yml

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin  #role name
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
...

$ kubectl apply -f cluster-role-binding.yml


# Get the bearer token
$ kubectl create token admin-user -n kubernetes-dashboard





==========================================================================================
2. ✅ Now apply this correct StatefulSet YAML (with clean volume and proper initContainers):
=========================================================================================

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: devopsexamapp
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: my-secret-pw
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      initContainers:
      - name: init-mysql
        image: busybox
        command: ["sh", "-c", "rm -rf /var/lib/mysql/*"]
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: mysql-persistent-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ebs-sc
      resources:
        requests:
          storage: 5Gi
This uses an initContainer to wipe the data directory each time a new pod is created, ensuring clean initialization.

======================================================================================
Option 2: Use an initContainer to initialize only if empty
If you want to initialize only once, use a safe condition:
======================================================================================
initContainers:
- name: init-mysql-safe
  image: busybox
  command:
    - sh
    - -c
    - |
      if [ ! -d /var/lib/mysql/mysql ]; then
        echo "Initializing database...";
        # possibly copy seed data or do nothing
      else
        echo "Data exists, skipping init.";
      fi
  volumeMounts:
    - name: mysql-persistent-storage
      mountPath: /var/lib/mysql
This checks if MySQL is already initialized by looking for the mysql system DB folder.




apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: devopsexamapp
spec:
  serviceName: "mysql"
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
        - name: init-mysql
          image: busybox
          command: ["sh", "-c", "rm -rf /var/lib/mysql/*"]
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
      containers:
        - name: mysql
          image: mysql:5.7
          ports:
            - containerPort: 3306
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "rootpass"
            - name: MYSQL_DATABASE
              value: "devops_exam"
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: mysql-persistent-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi


where the initContainer only removes MySQL data if it's not already initialized, preventing data loss on pod restart:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: devopsexamapp
spec:
  serviceName: "mysql"
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
        - name: init-mysql
          image: busybox
          command:
            - sh
            - -c
            - |
              if [ ! -d /var/lib/mysql/mysql ]; then
                echo "Initializing: empty data directory";
              else
                echo "MySQL data already exists. Skipping init.";
                exit 0
              fi
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
      containers:
        - name: mysql
          image: mysql:5.7
          ports:
            - containerPort: 3306
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "rootpass"
            - name: MYSQL_DATABASE
              value: "devops_exam"
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: mysql-persistent-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi








============================================================================================================================================================================================
step-by-step practical guide to dynamically provision a PostgreSQL database with a StatefulSet and show you how to recover using a backup, in case the pod or PVC is deleted.
============================================================================================================================================================================================
🧱 STEP 1: Create Namespace

kubectl create namespace devopsexamapp
📦 STEP 2: Create StorageClass (if not already present)
Save this as 1-storageclass.yaml:


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: postgres-storage-class
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
bash
Copy
Edit
kubectl apply -f 1-storageclass.yaml
🛠️ STEP 3: Create Headless Service
Save this as 2-postgres-service.yaml:



apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: devopsexamapp
spec:
  clusterIP: None
  selector:
    app: postgres
  ports:
    - port: 5432
      targetPort: 5432

kubectl apply -f 2-postgres-service.yaml
🗄️ STEP 4: Create StatefulSet with PVC
Save this as 3-postgres-statefulset.yaml:


apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: devopsexamapp
spec:
  serviceName: "postgres"
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:14
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: "devops_exam"
            - name: POSTGRES_USER
              value: "admin"
            - name: POSTGRES_PASSWORD
              value: "adminpass"
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: postgres-storage-class
        resources:
          requests:
            storage: 1Gi

kubectl apply -f 3-postgres-statefulset.yaml
🔍 STEP 5: Verify PV/PVC/POD

kubectl get pvc -n devopsexamapp
kubectl get pv
kubectl get pods -n devopsexamapp
You should see:

PVC in Bound state.

PV automatically created.

Pod running.

🧪 STEP 6: Create a Table & Insert Data

kubectl exec -it postgres-0 -n devopsexamapp -- bash
psql -U admin -d devops_exam
Inside psql:

sql
Copy
Edit
CREATE TABLE sample(id INT, name TEXT);
INSERT INTO sample VALUES (1, 'Kiran');
\q
💾 STEP 7: Backup the Data

kubectl exec -it postgres-0 -n devopsexamapp -- bash
pg_dump -U admin -d devops_exam > /tmp/devops_exam.sql
exit
kubectl cp devopsexamapp/postgres-0:/tmp/devops_exam.sql ./devops_exam.sql

❌ STEP 8: Simulate Crash
Delete the namespace or PVC:


kubectl delete pvc -n devopsexamapp --all
kubectl delete pod postgres-0 -n devopsexamapp
PVC will be recreated automatically with an empty volume.

🔁 STEP 9: Restore Data

kubectl cp ./devops_exam.sql devopsexamapp/postgres-0:/tmp/devops_exam.sql
kubectl exec -it postgres-0 -n devopsexamapp -- bash
psql -U admin -d devops_exam -f /tmp/devops_exam.sql
✅ STEP 10: Verify Data

psql -U admin -d devops_exam
SELECT * FROM sample;
You should see your data back.


=====================================================================================================================================================================
step-by-step guide to prevent data loss in Kubernetes StatefulSets (like PostgreSQL) using dynamic provisioning with a StorageClass and setting up automatic backups.
====================================================================================================================================================================
✅ Step 1: Create a StorageClass with Retain Policy

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3-retain
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
Save this as storageclass-retain.yaml and apply:


kubectl apply -f storageclass-retain.yaml
✅ Step 2: Deploy PostgreSQL StatefulSet with the above StorageClass

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: devopsexamapp
spec:
  serviceName: "postgres"
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:13
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: devops_exam
            - name: POSTGRES_USER
              value: admin
            - name: POSTGRES_PASSWORD
              value: adminpass
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp3-retain
        resources:
          requests:
            storage: 1Gi
Save as postgres-statefulset.yaml and apply:


kubectl apply -f postgres-statefulset.yaml
✅ Step 3: Backup with pg_dump using CronJob

apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: devopsexamapp
spec:
  schedule: "0 * * * *"  # hourly
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: pg-dump
              image: postgres:13
              env:
                - name: PGPASSWORD
                  value: adminpass
              command: ["sh", "-c"]
              args:
                - |
                  pg_dump -h postgres-0.postgres.devopsexamapp.svc.cluster.local -U admin -d devops_exam > /backup/devops_exam.sql
              volumeMounts:
                - name: backup-volume
                  mountPath: /backup
          restartPolicy: OnFailure
          volumes:
            - name: backup-volume
              persistentVolumeClaim:
                claimName: postgres-backup-pvc
You need a separate PVC for /backup. Create it as needed.

✅ Step 4: Test Pod Deletion with Volume Retention

kubectl delete pod postgres-0 -n devopsexamapp
Then confirm:


kubectl get pv
It should still show Bound and not deleted.



=======================================================================================
how to correctly handle PostgreSQL StatefulSet with dynamic provisioning, especially focusing on your case where PVCs remain stuck in Pending after deletion — applying everything I explained previously specifically to PostgreSQL.
========================================================================================
✅ Step-by-Step Guide for PostgreSQL with Dynamic Provisioning
🧱 1. Create the StorageClass
Here’s a recommended StorageClass for AWS EBS CSI:


# storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: postgres-gp3
provisioner: ebs.csi.aws.com
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
Apply it:


kubectl apply -f storageclass.yaml
📦 2. Create a Headless Service for PostgreSQL
This is required for StatefulSet DNS:


# postgres-headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: devopsexamapp
spec:
  ports:
    - port: 5432
  clusterIP: None
  selector:
    app: postgres
🏗️ 3. Create the PostgreSQL StatefulSet
This is the core StatefulSet with volume claim referencing the StorageClass:


# postgres-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: devopsexamapp
spec:
  serviceName: "postgres"
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:13
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: devops_exam
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: postgrespass
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: postgres-gp3
        resources:
          requests:
            storage: 1Gi
🔍 Key part: storageClassName: postgres-gp3

🛠️ 4. Apply in Correct Order

kubectl create namespace devopsexamapp
kubectl apply -f storageclass.yaml
kubectl apply -f postgres-headless-svc.yaml
kubectl apply -f postgres-statefulset.yaml
🔍 5. Verify PVC and Pod Status

kubectl get pvc -n devopsexamapp
kubectl get pv
kubectl get pods -n devopsexamapp
Expected result:

PVC status → Bound

PV → created automatically

Pod → Running

💥 What If Pod, PVC, or PV Are Deleted?
If you delete everything, including PVs and PVCs, and StorageClass remains, do this:

🔁 Cleanup Procedure

kubectl delete statefulset postgres -n devopsexamapp
kubectl delete pvc -l app=postgres -n devopsexamapp
kubectl delete pv --all  # Optional if manually created
kubectl delete svc postgres -n devopsexamapp
kubectl delete namespace devopsexamapp



Ensure:

1)No finalizers stuck on PVCs (kubectl patch if needed)

Check finalizers:
kubectl get pvc -n devopsexamapp -o yaml

If you see this under metadata:

finalizers:
  - kubernetes.io/pvc-protection

Remove stuck finalizer (force delete PVC):
kubectl patch pvc <pvc-name> -n devopsexamapp -p '{"metadata":{"finalizers":null}}' --type=merge

Or forcibly remove all:

kubectl get pvc -n devopsexamapp | awk 'NR>1 {print $1}' | while read pvc; do
kubectl patch pvc "$pvc" -n devopsexamapp -p '{"metadata":{"finalizers":null}}' --type=merge
done


corresponding PV is still stuck in Terminating state. This sometimes happens when the volume has a finalizer attached as well.

Here’s how to remove the finalizer from the PersistentVolume (PV) manually:

🧹 Remove Finalizer from Stuck PV
1. Edit the stuck PV manually

kubectl edit pv pvc-ae04f9bf-2af1-4e92-b55e-b91273f7d221
2. In the editor that opens, scroll to this section:

finalizers:
  - kubernetes.io/pv-protection
3. Delete the finalizers section, then save and exit the editor.
✅ Now Verify:

kubectl get pv
The stuck PV should now be deleted automatically.


CSI drivers are running

kubectl get pods -n kube-system | grep ebs

If they’re not running, you need to install the AWS EBS CSI driver:
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.28"



Then, recreate from step 1 again.




=====================================================================================
Scenario: PostgreSQL with Persistent Data on EBS (gp3)
=====================================================================================
✅ Objective:
Create a StorageClass with Retain.

Deploy PostgreSQL StatefulSet.

Verify data is retained after StatefulSet deletion.

Restore data by re-attaching the PV to a new PVC.

🔧 Step-by-Step Guide
Step 1: Create a StorageClass with Retain
storageclass-retain.yaml


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3-retain
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer

kubectl apply -f storageclass-retain.yaml
Step 2: Deploy PostgreSQL StatefulSet
postgres-statefulset.yaml


apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: devopsexamapp
spec:
  serviceName: "postgres"
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:15
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: devops_exam
            - name: POSTGRES_USER
              value: admin
            - name: POSTGRES_PASSWORD
              value: adminpass
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp3-retain
        resources:
          requests:
            storage: 2Gi

kubectl apply -f postgres-statefulset.yaml
Step 3: Insert Some Data into PostgreSQL
Log into the pod:


kubectl exec -it postgres-0 -n devopsexamapp -- bash
psql -U admin -d devops_exam
Insert test data:


CREATE TABLE students (id SERIAL PRIMARY KEY, name TEXT);
INSERT INTO students (name) VALUES ('Alice'), ('Bob');
\q
exit
Step 4: Delete the StatefulSet Only

kubectl delete statefulset postgres -n devopsexamapp
Check PVC still exists:


kubectl get pvc -n devopsexamapp
Step 5: Delete the PVC (Simulate accident)

kubectl delete pvc postgres-storage-postgres-0 -n devopsexamapp
Now check PV status:


kubectl get pv
It should show Released.

Step 6: Patch PV to Remove ClaimRef

kubectl patch pv <pv-name> -p '{"spec":{"claimRef": null}}'
Replace <pv-name> with the actual PV name, like pvc-xxxxx.

Step 7: Recreate PVC Manually and Rebind to Existing PV
postgres-pvc-reuse.yaml


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-storage
  namespace: devopsexamapp
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  volumeName: <pv-name>      # Same as the PV name
  storageClassName: ""       # Empty to match the retained PV

kubectl apply -f postgres-pvc-reuse.yaml
Step 8: Recreate the StatefulSet
Reapply the StatefulSet YAML (same as step 2).


kubectl apply -f postgres-statefulset.yaml
Step 9: Verify Data
Log in again:


kubectl exec -it postgres-0 -n devopsexamapp -- psql -U admin -d devops_exam
Check the data:


SELECT * FROM students;



