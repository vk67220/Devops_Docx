---
apiVersion: v1
kind: Pod #Kind is written as Pod, because we are creating a POD here
metadata:
  name: javawebapppod #its a pod name
  labels: #to identify pods in the cluster, we will give labels. Label is an ID for the Pod
    app: javawebapp #pod label
spec: #here we will specify what we want to do with pod
  containers: #here we are creating a container
    - name: javawebappcontainer #container name
      image: kastrov/javaappkastro
      ports: #port mapping
        - containerPort: 8080 #any number can be given
---

---
apiVersion: v1
kind: Service
metadata: 
  name: javawebappsvc #Service name. Any name can be given
spec: # used to identify the lable of a pod
  type: NodePort
  selector: # it is used to identify the pod by using pod label
    app: javawebapp
  ports: 
  - port: 80 #serivce cluster host port
    targetPort: 8080 #container port
---

Manifest file (2 in 1): Pod and service manifests are in the same file
---
apiVersion: v1
kind: Pod
metadata: 
  name: javawebapppod
  labels:
    app: javawebapp
spec:
  containers:
    - name: javawebappcontainer
      image: kastrov/zomato
      ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata: 
  name: javawebappsvc
spec:
  type: NodePort
  selector: 
    app: javawebapp
  ports: 
    - port: 80
      targetPort: 3000
      nodePort: 31877 #any number can be given but it has to be b/w 30000 and 32767
---

Create the pod
Check in which worker node the pod is created
Open the port given in above service manifest file of worker node security group
access the app: http://13.127.196.123:31877/


Namespace with service loadbalancer
---
apiVersion: v1
kind: Namespace
metadata:
   name: kastrons
---
apiVersion: v1
kind: Pod
metadata:
  name: javawebapppod
  labels:
    name: javawebapp
  namespace: kastrons 
spec:
  containers:
  - name: javawebappcontainer
    image: kastrov/swiggy
    ports:
     - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
  namespace: kastrons
spec:
  type: LoadBalancer
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31700
 ---

Replication Controller

---
apiVersion: v1
kind: ReplicationController
metadata:
  name: javawebapprc
spec:
  replicas: 3
  selector:
    app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
        - name: javawebappcontainers
          image: kastrov/swiggy
          ports:
            - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 30999
 ---

ReplicaSet

---
apiVersion: v1
kind: RelicaSet 
metadata:
  name: javawebapprs
spec:
   replicas: 3
   selector:
     matchLabels:
       app: javawebapp
     template:
       metadata:
         name: javawebapppod
         labels:
           app: javawebapp
       spec:
         containers:
           name: javawebappcontainer
           image: kastrov/bms
           ports:
             containerport: 3000
---
apiVersion: v1
kind: apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector: 
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31000
---


Deployment

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2
  strategy:
    type: ReCreate
  selector:
    matchLabels:
      app: javawebapp
  template:
    metadata:
      labels:
        app: javawebapp
    spec:  
      containers:
      - name: javawebappcontainer
        image: kastrov/zomato
        ports:
        - containerPort: 3000

---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 3000
    nodePort: 31000
---

above script do statefulset 


Helm charts:-

create a testdemo folder insdie create a folder called template
testdemo
   |------chart.yml
   |------values.yml 
   |------template
            |--------- nginxpod.yml ----manifest file
nginxpod.yml create in template folder


apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       image: nginx:latest
     - conatinerPort: 80


apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       image: {{ .Values.nginx.tag }}
     - conatinerPort: {{ .Values.nginx.nginxport }}

apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       {{- with.Values.nginx }}
       image: nginx: {{ .tag }}
     - conatinerPort: {{ .nginxport }}
     {{- end}}


if we comment tag in values-prod.yml file

apiVersion: v1
kind: pod
metadata:
   name: webapp
   labels:
      a: b
spec:
   containers: 
     - name: test
       {{- with.Values.nginx }}
       image: nginx: {{ .tag | default "test" }}  if i comment tag details on values-prod.yml it will take test as tag name
     - conatinerPort: {{ .nginxport }}
     {{- end}}

values.yml

nginx:
 tag: latest
 nginxport: 80

values-prod.yml

nginx:
 tag: v1
 nginxport: 8080

chart.yml

apiVersion: v2
name: nginx-deploy
description: "Deploying nginx chart to K8S cluster"
version: 0.0.1
appVersion: latest


How to setup HPA in K8S Cluster?
1. Install metric server/API
2. Deploy a sample app
3. Create Service
4. Deploy HPA
5. Increase load - load generator
6. Monitor HPA events

To increase the load on server manually
kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://hpa-demo-deployment; done"



------------------------------------
K8S Web Dashboard
------------------------------------
1. Kubectl (CLI)
2. Web Dashboard (UI based)

======================
K8S Web Dashboard Setup
======================
Connect to Master Node
kubectl delete all --all

#Lets download and apply the yml file to set web dashboard
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml

# The above yml file will create a namespace called "kubernetes-dashboard," where the pods get created.
# Lets see the pods that are running in the namespace
$ kubectl -n kubernetes-dashboard get pods -o wide
#You can see 2 pods got created.

# Lets see the service that is created in the namespace
$ kubectl -n kubernetes-dashboard get svc

# Edit k8s dashboard service and change it to NodePort from Cluster IP
$ kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard

Note: Check kubernetes-dashboard node port and enable that Node PORT in security group of worker node
#To see the details of port no.
$ kubectl -n kubernetes-dashboard get svc
#You will see port no. as 443:30077/TCP
Goto worker node VM and edit the inbound SG rules and add the port 30077 with custom TCP and anywhere IPv4
Goto worker node VM and edit the inbound SG rules and add the port 443 with HTTPS and anywhere IPv4

# Check in which node kubernete-dashboard POD is running
$ kubectl get pods -o wide -n kubernetes-dashboard


# Access k8s web ui dashboard using below URL

URL : https://node-public-ip:30077/
#You will see the web dashboard

#'Check' Token. In order to get the token we need to work with RBAC (Roll Based Access). So we need to create RBAC i.e we need to create an user and for that user we need to give access for web dashboard.

#Lets create admin user with below yml

$ vi create-user.yml

#Copy and paste the below yml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard  #We are creating the 'admin-user' under the "kubernetes-dashboard" namespace.
...

$ kubectl apply -f create-user.yml
#You can see admin-user created.

Now we need to create the role and bind that role to the admin-user

# create cluster role binding
$ vi cluster-role-binding.yml

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin  #role name
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
...

$ kubectl apply -f cluster-role-binding.yml


# Get the bearer token
$ kubectl create token admin-user -n kubernetes-dashboard





==========================================================================================
2. âœ… Now apply this correct StatefulSet YAML (with clean volume and proper initContainers):
=========================================================================================

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: devopsexamapp
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: my-secret-pw
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      initContainers:
      - name: init-mysql
        image: busybox
        command: ["sh", "-c", "rm -rf /var/lib/mysql/*"]
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: mysql-persistent-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ebs-sc
      resources:
        requests:
          storage: 5Gi
This uses an initContainer to wipe the data directory each time a new pod is created, ensuring clean initialization.

======================================================================================
Option 2: Use an initContainer to initialize only if empty
If you want to initialize only once, use a safe condition:
======================================================================================
initContainers:
- name: init-mysql-safe
  image: busybox
  command:
    - sh
    - -c
    - |
      if [ ! -d /var/lib/mysql/mysql ]; then
        echo "Initializing database...";
        # possibly copy seed data or do nothing
      else
        echo "Data exists, skipping init.";
      fi
  volumeMounts:
    - name: mysql-persistent-storage
      mountPath: /var/lib/mysql
This checks if MySQL is already initialized by looking for the mysql system DB folder.




apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: devopsexamapp
spec:
  serviceName: "mysql"
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
        - name: init-mysql
          image: busybox
          command: ["sh", "-c", "rm -rf /var/lib/mysql/*"]
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
      containers:
        - name: mysql
          image: mysql:5.7
          ports:
            - containerPort: 3306
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "rootpass"
            - name: MYSQL_DATABASE
              value: "devops_exam"
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: mysql-persistent-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi


where the initContainer only removes MySQL data if it's not already initialized, preventing data loss on pod restart:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: devopsexamapp
spec:
  serviceName: "mysql"
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
        - name: init-mysql
          image: busybox
          command:
            - sh
            - -c
            - |
              if [ ! -d /var/lib/mysql/mysql ]; then
                echo "Initializing: empty data directory";
              else
                echo "MySQL data already exists. Skipping init.";
                exit 0
              fi
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
      containers:
        - name: mysql
          image: mysql:5.7
          ports:
            - containerPort: 3306
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "rootpass"
            - name: MYSQL_DATABASE
              value: "devops_exam"
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: mysql-persistent-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi

